{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "UDDeGDdqX1-n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import cuda\n",
        "import sys\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'allenai/scibert_scivocab_uncased'"
      ],
      "metadata": {
        "id": "t6tGSgX3X8jk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "LMTokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "LMModel = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "T0t0HrkOX9I5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = pd.read_csv('/content/train.csv', sep=',', names=['CGT','CDT','CC','label'])\n",
        "testing_dataset = pd.read_csv('/content/validation.csv', sep=',', names=['CGT','CDT','CC','label'])\n",
        "\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 4\n",
        "LEARNING_RATE = 0.00001\n",
        "drop_out = 0.1\n",
        "EPOCHS = 1\n",
        "tokenizer = LMTokenizer"
      ],
      "metadata": {
        "id": "Pi__4pqhYCM9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file_name = f\"{sys.argv[5]}_{model_name.split('/')[-1]}_{TRAIN_BATCH_SIZE}_{LEARNING_RATE}_{drop_out}.txt\" if len(sys.argv) > 5 else \"output.txt\"\n",
        "file = open(output_file_name,'w')"
      ],
      "metadata": {
        "id": "FiSV9vYQYHUm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        CGT = str(self.data.CGT[index])\n",
        "        CGT = \" \".join(CGT.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            CGT,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        CGT_ids = inputs['input_ids']\n",
        "        CGT_mask = inputs['attention_mask']\n",
        "\n",
        "\n",
        "        CDT = str(self.data.CDT[index])\n",
        "        CDT = \" \".join(CDT.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            CDT,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        CDT_ids = inputs['input_ids']\n",
        "        CDT_mask = inputs['attention_mask']\n",
        "\n",
        "\n",
        "        CC = str(self.data.CC[index])\n",
        "        # print(CC)\n",
        "        CC = \" \".join(CC.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            CC,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        CC_ids = inputs['input_ids']\n",
        "        CC_mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'CGT_ids': torch.tensor(CGT_ids, dtype=torch.long),\n",
        "            'CGT_mask': torch.tensor(CGT_mask, dtype=torch.long),\n",
        "\n",
        "            'CDT_ids': torch.tensor(CDT_ids, dtype=torch.long),\n",
        "            'CDT_mask': torch.tensor(CDT_mask, dtype=torch.long),\n",
        "\n",
        "            'CC_ids': torch.tensor(CC_ids, dtype=torch.long),\n",
        "            'CC_mask': torch.tensor(CC_mask, dtype=torch.long),\n",
        "\n",
        "            'targets': torch.tensor(self.data.label[index], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "ccf2sOXDYJog"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = Triage(testing_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n"
      ],
      "metadata": {
        "id": "pikK5qDRYM6g"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LMClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LMClass, self).__init__()\n",
        "        self.l1 = LMModel\n",
        "        self.lstm = torch.nn.LSTM(768, 768, bidirectional=True, dropout=drop_out, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(768 * 2, 2)\n",
        "        self.act = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, data):\n",
        "        dimension = 768\n",
        "\n",
        "        input_ids = data['CC_ids'].to(device, dtype = torch.long)\n",
        "        attention_mask = data['CC_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        ## print (\"output_1\")\n",
        "        #print (output_1.shape)\n",
        "        hidden_state1 = output_1[0]\n",
        "        pooler = hidden_state1[:, 0]\n",
        "        ## print (\"hidden state  1 \")\n",
        "        ## print (hidden_state1.shape)\n",
        "        ## print (\"pooler\")\n",
        "        ## print (pooler.shape)\n",
        "        packed_output, (hidden, cell) = self.lstm(hidden_state1)\n",
        "\n",
        "        text_len = 512\n",
        "        out_forward =  packed_output[range(len(packed_output)), text_len - 1, :dimension]\n",
        "        out_reverse = packed_output[:, 0, dimension:]\n",
        "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
        "\n",
        "        ## print (\"out_reduced shape\", out_reduced.shape)\n",
        "\n",
        "\n",
        "        dense_outputs=self.fc(out_reduced)\n",
        "        ## print (\"dense shape\", dense_outputs.shape)\n",
        "\n",
        "        outputs=self.act(dense_outputs)\n",
        "        ## print(outputs.shape)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "xvyHqEnFYPMF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nếu bạn dùng BCEWithLogitsLoss và muốn giữ hai đầu ra hoặc loss_function = torch.nn.CrossEntropyLoss()\n",
        "# Sử dụng BCEWithLogitsLoss cho phân loại nhị phân với nhãn one-hot encoding\n",
        "loss_function = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Khi tính toán loss, cần chuyển đổi nhãn sang one-hot encoding\n",
        "targets = torch.nn.functional.one_hot(targets, num_classes=2).float()\n",
        "loss = loss_function(outputs, targets)"
      ],
      "metadata": {
        "id": "pZ2s8j_ZyDY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LMClass()\n",
        "model.to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvx4uC9IYUYv",
        "outputId": "ac54da6c-3115-4559-d3f0-0b1c5c482b0b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(data)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    file.write(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}\\n')\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}\\n')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    file.write(f\"Training Loss Epoch: {epoch_loss}\\n\")\n",
        "    file.write(f\"Training Accuracy Epoch: {epoch_accu}\\n\")\n",
        "    file.write(\"\\n\")\n",
        "    return"
      ],
      "metadata": {
        "id": "4a9UHaWOhFXE"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; tr_loss = 0\n",
        "    nb_tr_steps =0\n",
        "    nb_tr_examples =0\n",
        "    pred = []\n",
        "    act = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(data).squeeze()\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accu(big_idx, targets)\n",
        "            pred += big_idx.tolist()\n",
        "            act += targets.tolist()\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    file.write(f\"Validation Loss Epoch: {epoch_loss}\\n\")\n",
        "    file.write(f\"Validation Accuracy Epoch: {epoch_accu}\\n\")\n",
        "    mf1 = f1_score(act, pred, average='macro')\n",
        "    file.write(f\"Validation Macro F1: {mf1}\\n\")\n",
        "    return mf1,epoch_accu\n"
      ],
      "metadata": {
        "id": "T9xhMhAHhGdC"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_mf1 = 0\n",
        "best_epoch = 0\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)\n",
        "    mf1,acc = valid(model, testing_loader)\n",
        "    if mf1 > best_mf1:\n",
        "        best_mf1 = mf1\n",
        "        best_acc = acc\n",
        "        best_epoch = epoch+1\n",
        "\n",
        "    file.write(\"\\n\")\n",
        "\n",
        "file.write(\"Best \\nAccuracy: {0} \\nMacro F1 Score: {1}\\nAt Epoch: {2}\\n\".format(best_acc,best_mf1,best_epoch))\n",
        "\n"
      ],
      "metadata": {
        "id": "1cj4IszPhJlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9edee1ad-b272-4f73-cb93-658dbee6d072"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 0: 57.44\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file.close()\n"
      ],
      "metadata": {
        "id": "kfuQ9SsU0fas"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}